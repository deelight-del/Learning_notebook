{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6dfbaa0",
   "metadata": {},
   "source": [
    "## Machine Learning Concepts\n",
    "### 09/07/2022 - Classification Metrics: Accuracy, Precision, Recall.\n",
    "\n",
    "> Supervised Machine Learning is split into primarily regression and classification. In regression, there are several metrics including the most popular ones as mean squared error and mean absolute error.\n",
    "\n",
    "> In classification these types of metrics are not applicable because they take the absolute individual diiferences between a predicted value and true value. However in classification, our target values are discrete variables and not continous variables that affords us to do meaningful substraction.\n",
    "\n",
    "### Accuracy.\n",
    "\n",
    ">- In order to continue with this notebook, we would be considering simple binary classification in which the target labels are either true or false.\n",
    ">- In accuracy, we calcualte the error metric of a particular model as the number of the accurately predicted labels over the total number of labels. An accuracy of 0.8 suggests that the model accurately made 8 out of 10 predictions accurately and it is good for considering an overall metric of a model performance.\n",
    ">- However, accuracy might be deceitful in telling us the whole story especially when dealing with an imbalance training set. Take for example, there are 100 observations and 90 of the observations marks false for the label to be predicted and the 10 for the true class. If a naive model which **always** predict the mode class of the observation set is evaluated using the accuracy score, it will have a 90% accuracy despite being a naive model and not able to actually catch a single true label.\n",
    "\n",
    "     This leads us to the concept of recall and precision. But before that it is important to make refeence to the confusion matrix with image below. In a confusion matrix, the actual labels of the observation is marked on the Y-axis and the predicted labels is marked on the X-axis. (Note, this is axis description is different from image below but consistent with the nConfusionMatrixDisplay of sklearn.metrics module)\n",
    "\n",
    "> ![David%20a.jpg](Images/Confusion-matrix-and-related-performance-measures.png)\n",
    "\n",
    ">- From the image we identify Things like TP(True Positive), FN(False Negatives), FP(False Postives) and True Negatives(TN)\n",
    "The colour green is to mark which predicted observations are correct (TP and TN) and the Red vice versa.\n",
    "\n",
    "### Precision\n",
    "> Precision measures the preciseness of a model in respect to the positive class. Precision refers to how much we can trust a model's positive prediction. From the formula in the image above, a model seeking for higher precision would look for a way to reduce its false positives and possibly eliminate it to acheive a precion of 1. \n",
    "\n",
    "> What this means and how it affect the model is that until the model is (very) certain that a particular observation is positive class, it would rather classify it as a negative class. In this high precision process, the model runs risk of classifying positive classes as negative classes because it wants to be very certain of its predictions.\n",
    "\n",
    "### Recall\n",
    "\n",
    "> If a model is trying to acheive a high precision there is absolute tendency to increase the number of False Negatives, i.e, the model begins to assign \"unsure\" but actual true/positive labels as negative labels. FN is thus every positive class that was failed to be identified by the model.\n",
    "\n",
    "> Thus a model seeking for a high recall, ability for the model to catch all the positive observation at the slightest guess, would be a model that seeks to reduce its False Negatives and in such process of reducing the False Negatives, begins to always want to assign a model as a positive class. This high recall model begins to also increase the False Positives because it will want to predict a positive class at every observation it gets. Thus as a data scientist, one must be able to know which trade off you are willing to sacrifice to make certain model designs.\n",
    "\n",
    "> A naive way to remember the difference between precision and recall is to think of precision as high quality model and recall as high quanitity model.\n",
    "\n",
    "### Threshold\n",
    "\n",
    "> Generally the threshold is the threshold at which a model predicts a class as positive or negative. It stands at default 0.5 for most models and what this means is that for model probability prediction greater or equal 0.5, the model will predict TRUE and for probability prediction less than 0.5, the model will predicf FALSE. In order to adjust the recall and precision of a model, we adjust the threshold of the model. Increasing the threshold increases the precision and lowers the recall, while lowering the threshold increases the recall and reduces the precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
