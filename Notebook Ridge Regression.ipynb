{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cb25d1",
   "metadata": {},
   "source": [
    "## Machine Learning Concepts\n",
    "\n",
    "### 25/06/2022 - Ridge Regression, Bias and Variance.\n",
    "\n",
    "> Generally Linear model are bias models that are very primitive.\n",
    "Primitivity is a characteristics of bias models. What \n",
    "characterizes a model as bias is how \"insensitive\" they \n",
    "are to change i.e. their model parameters are rigid, often easy\n",
    "to read but not always a bad thing.\n",
    "\n",
    "> High variance models however are models that are too \"complex\"\n",
    "and too \"sensiive\" to every data point(including noise) of a \n",
    "training data till the extent that they begin to overfit. \n",
    "An effect of this that they would not generalize well with other\n",
    "datasets that they have not seen.\n",
    "\n",
    "> With this in check, especially the aspect of sensitivity,\n",
    "relating to bias and variance model. Let us compare Ridge \n",
    "Regression model and Linear Regression models.\n",
    "\n",
    ">- What Ridge Models does underneath the hood is that it penalizes\n",
    "a linear model when it becomes so steep using the formula:\n",
    "> $ (sum of squared errors) + lambda + (slope)^2 $\n",
    "Thus favouring a lower steep model over a high steep model.\n",
    "\n",
    ">- A high steep model is a model **very sensitive**\n",
    "to y-output change on little x-input change. For example\n",
    "1-x change resulting in 3-y change is more steep than 1-x \n",
    "change resulting in 1.5-y change.\n",
    "A high steep model corresponds to a high variance model and \n",
    "low steep corresponds to low variance model.\n",
    "\n",
    ">- This ridge Regression penalty reduces how a model is sensitive to an axis\n",
    "and is very useful for high dimensional data points whch might tend to\n",
    "extremely favour one feature above other features. At high-D datapoints,\n",
    "The ridge model penalizes a linear model for becoming too step towards\n",
    "a particular feature.\n",
    "\n",
    "> Thus in respect to bias and variance between the Ridge and Linear Regression \n",
    "model, the following is valid:\n",
    "   >- The ridge model is more bias than the linear model. It is rigid and insensitive to y-output change on x-input change\n",
    "   Basically the ridge penalty adds more bias to the linear model by definition.\n",
    "   >- The ridge model reduces the variance and overfitting because it is less prone to the noise in the dataset and this increases its generalizability and usefulness especially with high-D datapoints.\n",
    "   \n",
    "\n",
    "[Youtube Link from statquest for better explanations](https://youtu.be/Q81RR3yKn30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
